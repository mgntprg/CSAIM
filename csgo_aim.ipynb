{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"csgo_aim.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPVfmPZlvMoYs2BVYgw8rsL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"uV1SjxcQ8nUD","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413007,"user_tz":420,"elapsed":367,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bMF_S4mp8ADx","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413009,"user_tz":420,"elapsed":357,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["import tensorflow as tf\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGe74zmq9WtN","colab_type":"text"},"source":["Use tf.dataset to get images and sort out all the directory stuff"]},{"cell_type":"markdown","metadata":{"id":"79hPriIQ9T0Z","colab_type":"text"},"source":["Import pre-trained model and get the necessary weights"]},{"cell_type":"code","metadata":{"id":"zLxl_7M89QVK","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413009,"user_tz":420,"elapsed":348,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["import os\n","data_root = 'gdrive/My Drive/ML/Projects/CSGO_aim/images'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oD7VBM87J83","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413010,"user_tz":420,"elapsed":338,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["BUFFER_SIZE = 500\n","BATCH_SIZE = 8\n","IMG_WIDTH = 448\n","IMG_HEIGHT = 448"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2c-YBUOD6uQQ","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413011,"user_tz":420,"elapsed":327,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["def load(image_file):\n","  # height, width, channels\n","  image = tf.io.read_file(image_file)\n","  image = tf.image.decode_jpeg(image)\n","\n","  input_image = tf.cast(image, tf.float32)\n","\n","  return input_image\n","\n","def resize(input_image, height, width):\n","  input_image = tf.image.resize(input_image, [height, width], \n","                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","  return input_image\n","\n","def normalize(input_image):\n","  input_image = (input_image / 127.5) - 1\n","  return input_image\n","\n","def load_image_train(image_file):\n","\n","  input_image = load(image_file)\n","  input_image = resize(input_image, IMG_WIDTH, IMG_HEIGHT)\n","  input_image = normalize(input_image)\n","\n","  return input_image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMbr-HVvS1G7","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413012,"user_tz":420,"elapsed":318,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["def convert_label(label):\n","  label = list(map(int, label))\n","  # x coordinate\n","  # 640 is training image width\n","  label[0] = IMG_HEIGHT * label[0]/640 \n","  label[1] = IMG_WIDTH * label[1]/640\n","  # width and height rescaling\n","  label[2] = IMG_WIDTH * (label[2]/640) \n","  label[3] = IMG_HEIGHT * (label[3]/640)\n","\n","  label = list(map(int, label))\n","\n","  return label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IvqNnqHCZk2U","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413012,"user_tz":420,"elapsed":308,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["import os\n","\n","imgs = []\n","bounds = []\n","\n","for n, image_file in enumerate(os.listdir('gdrive/My Drive/ML/Projects/CSGO_aim/images/')):\n","  count = 10\n","\n","  imgs.append(load_image_train(os.path.join('gdrive/My Drive/ML/Projects/CSGO_aim/images', image_file)))\n","\n","  line_no = 'images\\\\' + image_file\n","\n","  fr = open('gdrive/My Drive/ML/Projects/CSGO_aim/train.txt', 'r')\n","\n","  ll = []\n","\n","  for _, line in enumerate(fr):\n","    if line.split()[0] == str(line_no):\n","      for label in line.split()[1:]:\n","        count -= 1\n","        bounds.append(convert_label(label.split(',')))\n","\n","  # invalid bounding box, padding\n","  for l in range(count):\n","    bounds.append([-1, -1, -1, -1, -1])\n","  \n","  #bounds.append(ll)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8RkYKzNHeA2V","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413013,"user_tz":420,"elapsed":296,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["np.array(bounds).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ud9fiUCCdjVM","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413014,"user_tz":420,"elapsed":286,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["new_bounds = []\n","for i in bounds:\n","  for j in i:\n","      new_bounds.append(j)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1zWIsOFeeGw","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413015,"user_tz":420,"elapsed":276,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["np.array(new_bounds).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N3VxfxvqaSOS","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413016,"user_tz":420,"elapsed":265,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["new_bounds = np.array(new_bounds).reshape(n + 1, 10, 5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h1E07PgdSma6","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413016,"user_tz":420,"elapsed":254,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["tuples = []\n","for i, img in enumerate(imgs):\n","  #tuples.append((img, bounds[i]))\n","  tuples.append((img, new_bounds[i, :, :]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tbVhRq_PXPxK","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413017,"user_tz":420,"elapsed":243,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["tuples[0][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q1rL0U7Xxj_A","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413018,"user_tz":420,"elapsed":232,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["train_dataset = tf.data.Dataset.from_tensor_slices((imgs, new_bounds))\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n","train_dataset = train_dataset.batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2DdKl0gI_1_U","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413018,"user_tz":420,"elapsed":220,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["'''#image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n","#image_data = image_generator.flow_from_directory(data_root, target_size=IMAGE_SHAPE)\n","#train_dataset = tf.data.Dataset.list_files('gdrive/My Drive/ML/Projects/CSGO_aim/images' + '/*.jpg')\n","#train_dataset = tf.data.Dataset.from_tensor_slices(tf.data.dataset.)\n","\n","train_dataset = tf.data.Dataset.list_files('gdrive/My Drive/ML/Projects/CSGO_aim/images' + '/*.jpg')\n","train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n","train_dataset = train_dataset.batch(BATCH_SIZE)\n","\n","print(os.listdir('gdrive/My Drive/ML/Projects/CSGO_aim/images'))'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhO1IntZ6lFM","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413019,"user_tz":420,"elapsed":209,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["IMAGE_SHAPE = (448, 448)\n","B = 2 # 2 bounding boxes per grid cell\n","C = 1 \n","s = 3\n","\n","model = None\n","\n","if(os.path.exists('gdrive/My Drive/ML/Projects/CSGO_aim/save/training_1')):\n","  model = tf.keras.models.load_model(\"gdrive/My Drive/ML/Projects/CSGO_aim/save/training_1/cp.ckpt\")\n","  print(len(model.layers))\n","  \n","  #for layer in model.layers:\n","      #print(layer.get_weights())\n","else:\n","  inputs = tf.keras.Input(shape=IMAGE_SHAPE + (3, ))\n","  x = tf.keras.layers.SeparableConv2D(3, 2, strides=(1, 1), padding='same')(inputs)\n","  x = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(x)\n","  x = tf.keras.applications.MobileNetV2(#input_shape=IMAGE_SHAPE + (3, ),\n","                                                include_top=False, weights='imagenet')(x)\n","  x = tf.keras.layers.Conv2D(48, 4, strides=(1, 1), padding='same')(x)\n","  x = tf.keras.layers.Conv2D(64, 3, strides=(2, 2))(x)\n","  x = tf.keras.layers.Conv2D(128, 1, strides=(1, 1))(x)\n","  # When applied on top of a Conv2D layer, this is basically a 1x1 feature detector\n","  # with 2048 feature maps, except the feature detectors are not constrained (since \n","  # Conv2D layers impose more constraints)\n","  x = tf.keras.layers.Dense(1024)(x)\n","  x = tf.keras.layers.Dense(512)(x)\n","  # B is the # of bounding boxes per grid cell, 5 is the amount of things we predict, \n","  # x, y, w, h, and confidence\n","  output_layer_1 = tf.keras.layers.Dense(B * 5, activation='linear')(x)\n","  # and C is # of classes -- just 1 due to binary\n","  output_layer_2 = tf.keras.layers.Dense(C, activation='sigmoid')(x)\n","\n","  model = tf.keras.Model(inputs=inputs, outputs=[output_layer_1, output_layer_2])\n","  model.get_layer('mobilenetv2_1.00_224').trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ig4Bw_VbC7j0","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413020,"user_tz":420,"elapsed":197,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["for layer in model.layers:\n","  print(layer.name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e4HSGStwDyCT","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413020,"user_tz":420,"elapsed":185,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HdpSdUTiHN-6","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413021,"user_tz":420,"elapsed":172,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["sample_imgs = train_dataset.take(1)\n","\n","for X, y in sample_imgs:\n","\n","  print(np.array(model(X)[0]).shape)\n","  print(np.array(model(X)[1]).shape)\n","  print(np.array(y).shape)\n","  print(y)\n","  for item in X:\n","    print(np.array(item).shape)\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xVJzw0T5W-U","colab_type":"text"},"source":["We have an architecture which outputs a 7x7x11 grids, with the grids determining the bounding box coordinates, and the classification determining what class the object is\n","\n"]},{"cell_type":"code","metadata":{"id":"2BgSi1Ce5J7v","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413022,"user_tz":420,"elapsed":163,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["# In an sxs grid, this identifies which grid is responsible; only one bounding box per grid is responsible\n","# label expectation: [(x, y, w, h)] unnormalized\n","# returns list of responsible grids\n","def get_responsible_grids(input_img, labels, s): \n","  responsible_grids = []\n","\n","  for obj in labels:\n","    # x and y are the center of the object\n","    sw = obj[0] // (IMG_WIDTH / s) \n","    sh = obj[1] // (IMG_HEIGHT / s) \n","    responsible_grids.append((int(sw), int(sh)))\n","    \n","  return responsible_grids\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjGGRtfZoco2","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413022,"user_tz":420,"elapsed":151,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["get_responsible_grids(0, [(300, 300, 100, 100)], 3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z1NqGsDZExqY","colab_type":"text"},"source":["The ground truth needs to be made in such a way that bounding boxes which are not responsible are not penalized (the gradient does not affect them). This can be done by watching only the parameters which yield the \"responsible\" grids.  "]},{"cell_type":"markdown","metadata":{"id":"DwdC5zH9GHXV","colab_type":"text"},"source":["The hard part is figuring out how to implement selective gradients for only the parameters which yield responsible boxes. Based on the loss function: \n","\n","*   coordinate error and width/height error are only penalized for the RESPONSIBLE bounding box if an object's center appears in the grid\n","*   Confidence error is weighted more heavily if an object appears in the image (and should be 0 if the object does not appear in the image; furthermore, both boxes take the loss for confidence)\n","*   The classifier is only penalized if an object appears in the grid\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tHDZ7pKnHRzR","colab_type":"text"},"source":["Penalizing confidence: check if the grid which the bounding box belongs to is responsible - if not, then scale the confidence loss down by lambda \n","If the grid is responsible, find which of the bounding boxes is responsible by calculating the intersection over union"]},{"cell_type":"markdown","metadata":{"id":"8EP-o3qWg-0O","colab_type":"text"},"source":["Found an IOU utility function here: https://gist.github.com/meyerjo/dd3533edc97c81258898f60d8978eddc"]},{"cell_type":"code","metadata":{"id":"XijH6Bt1HQ50","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413023,"user_tz":420,"elapsed":140,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["# Intersection over union\n","# The parameters are not width and height, but rather the upper left and \n","# lower right coords\n","def bb_intersection_over_union(boxA, boxB):\n","    # determine the (x, y)-coordinates of the intersection rectangle\n","    xA = max(boxA[0], boxB[0])\n","    yA = max(boxA[1], boxB[1])\n","    xB = min(boxA[2], boxB[2])\n","    yB = min(boxA[3], boxB[3])\n","\n","    # compute the area of intersection rectangle\n","    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n","    if interArea == 0:\n","        return 0\n","    # compute the area of both the prediction and ground-truth\n","    # rectangles\n","    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n","    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n","\n","    # compute the intersection over union by taking the intersection\n","    # area and dividing it by the sum of prediction + ground-truth\n","    # areas - the interesection area\n","    iou = interArea / float(boxAArea + boxBArea - interArea)\n","\n","    # return the intersection over union value\n","    return iou"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RFOhRMVahrv0","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413023,"user_tz":420,"elapsed":128,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["bb_intersection_over_union([0, 0, 200, 200], [100, 100, 300, 300])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jk9p65HESwM7","colab_type":"text"},"source":["We need to create a function which normalizes the width and height prediction of the network between 0 and 1 so that all components equally contribute to the loss function. \n","\n","This function takes in a label, and outputs x-y from 0 to 1 in the range of the grid cell, and a width and height relative to the entire image."]},{"cell_type":"code","metadata":{"id":"LS4lZ_u6SfMR","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413024,"user_tz":420,"elapsed":119,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["# Given label NOT IN A LIST FORM\n","def normalize_label(label, s):\n","  responsible_grid = get_responsible_grids(0, [label], s)[0]\n","  \n","  x = label[0]\n","  y = label[1]\n","  w = label[2]\n","  h = label[3]\n","\n","  x = (x % (IMG_WIDTH // s)) / (IMG_WIDTH // s)\n","  y = (y % (IMG_WIDTH // s)) / (IMG_HEIGHT // s)\n","\n","  w /= IMG_WIDTH\n","  h /= IMG_HEIGHT\n","\n","  return x, y, w, h"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6i_oWEm4UU5q","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413025,"user_tz":420,"elapsed":107,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["normalize_label((64,64,50,50), 7)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6jCpe9_3pQGj","colab_type":"text"},"source":["For each label in the input image, figure out which grid is responsible and then figure out which bounding box is responsible\n","\n","One of the hard parts in this was normalizing the box coordinates for when the grid contains the bounding box\n","\n","A lot of issues here. I am only processing one image at a time in order to make the code simpler. However, tensorflow functions require whatever they are processing to have some shape which also accounts for the batch size. I mitigate most of that here by indexing in order to compute the majority of the loss function. When calculating binary crossentropy, I have to convert label[-1] in to batch tensor form (None, ...) as well as my predictions since I am directly using a tensorflow function."]},{"cell_type":"code","metadata":{"id":"vtg1YJkNBk9H","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413025,"user_tz":420,"elapsed":97,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["loss_object = tf.keras.losses.BinaryCrossentropy()\n","\n","# Simple sxs grid with x, y, w, h, confidence level, and class for each image\n","# The ground truth needs to be made in such a way that bounding boxes which \n","# are not responsible are not penalized, and same with the classification\n","\n","def compute_loss(predictions_1, predictions_2, input_img, labels, s):\n","  #ground_truth_output_1 = np.empty(shape=(None, s, s, B * 5))\n","  #ground_truth_output_2 = np.empty(shape=(None, s, s, C))\n","  \n","  # All bounding boxes in the grid\n","\n","  # contains responsible bounding box\n","  bounding_boxes = []\n","  indexes = []\n","  max_ious = []\n","  responsible_box = None\n","  loss = 0\n","\n","  lambda_coord = 5\n","  lambda_noobj = 0.5\n","\n","  # for each label\n","  for label in labels: \n","    if label[0] == -1:\n","      return loss\n","    # for each grid\n","    max_iou = 0 \n","\n","    for i in range(s):\n","      for j in range(s):\n","        # If this is a responsible grid/ an object is in this grid\n","        wh_loss = 0\n","        xy_loss = 0\n","        confidence_loss = 0\n","        classification_loss = 0\n","\n","        if get_responsible_grids(input_img, [label], s)[0] == (i, j): #) != -1:\n","          # for each bounding box\n","          for k in range(B):\n","            bb = predictions_1[0, i, j, k*5:(k+1)*5]\n","            #bounding_boxes.append(bb)\n","\n","            x1, y1 = bb[0] - bb[2] // 2, bb[1] - bb[3] // 2  \n","            x2, y2 = bb[0] + bb[2] // 2, bb[1] + bb[3] // 2  \n","            box1 = (x1, y1, x2, y2)\n","            labelx1, labely1 = label[0] - label[2] // 2, label[1] - label[3] // 2  \n","            labelx2, labely2 = label[0] + label[2] // 2, label[1] + label[3] // 2 \n","\n","\n","            #predicted_confidence = bb[(k+1)*5-1] # predicted confidence\n","            predicted_confidence = bb[(0+1)*5-1]\n","\n","            true_confidence_iou = bb_intersection_over_union((x1, y1, x2, y2), (labelx1, labely1, labelx2, labely2))\n","            \n","            if true_confidence_iou >= max_iou:\n","              bounding_boxes.append((i, j, k))\n","\n","              label = normalize_label(label, s)\n","              label = tf.convert_to_tensor(label, dtype=tf.float32)\n","\n","              # xy coordinate squared difference (centered)\n","              xy_loss = lambda_coord * ((bb[0] - label[0])**2 + (bb[1] - label[1])**2)\n","\n","              # in order to take the square root\n","              sqrt_w = tf.clip_by_value(bb[2], 0, 100)\n","              sqrt_h = tf.clip_by_value(bb[3], 0, 100)\n","\n","              #wh_loss = lambda_coord * ((tf.math.sqrt(bb[2]) - tf.math.sqrt(label[2]))**2 \n","              #                         + (tf.math.sqrt(bb[3]) - tf.math.sqrt(label[3]))**2)\n","              wh_loss = lambda_coord * (sqrt_w - tf.math.sqrt(label[2]))**2 + (sqrt_h - tf.math.sqrt(label[3]))**2\n","              # Confidence of responsible boxes\n","              confidence_loss = (true_confidence_iou - predicted_confidence) ** 2\n","\n","          # classification loss\n","          classification_loss = loss_object(tf.convert_to_tensor([label[-1]]), \n","                                            tf.convert_to_tensor([predictions_2[0, i, j, 0]]))\n","        else: \n","           # for each bounding box\n","          for k in range(B):\n","            bb = predictions_1[0, i, j, k*5:(k+1)*5]\n","            #bounding_boxes.append(bb)\n","\n","            x1, y1 = bb[0] - bb[2] // 2, bb[1] - bb[3] // 2  \n","            x2, y2 = bb[0] + bb[2] // 2, bb[1] + bb[3] // 2  \n","            box1 = (x1, y1, x2, y2)\n","            labelx1, labely1 = label[0] - label[2] // 2, label[1] - label[3] // 2  \n","            labelx2, labely2 = label[0] + label[2] // 2, label[1] + label[3] // 2 \n","\n","  \n","\n","            #predicted_confidence = bb[(k+1)*5-1] # predicted confidence\n","            predicted_confidence = bb[(0+1)*5-1]\n","\n","            true_confidence_iou = bb_intersection_over_union((x1, y1, x2, y2), (labelx1, labely1, labelx2, labely2))\n","            \n","            if true_confidence_iou >= max_iou:\n","              bounding_boxes.append((i, j, k))\n","\n","              label = normalize_label(label, s)\n","              label = tf.convert_to_tensor(label, dtype=tf.float32)\n","\n","              # Confidence of responsible boxes\n","              confidence_loss = lambda_noobj * ((true_confidence_iou - predicted_confidence) ** 2)\n","\n","          # classification loss\n","          classification_loss = 0\n","  \n","        # only one box is responsible for confidence\n","        loss += (confidence_loss + xy_loss + wh_loss + classification_loss)\n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oes89YFf80Ne","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413027,"user_tz":420,"elapsed":88,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["for sample in train_dataset.take(1):\n","  outputs = model(sample)\n","  # label must be passed in as a list of tuples\n","  loss = compute_loss(outputs[0], outputs[1], 0, [[50, 50, 100, 100, 0]], s)\n","\n","print(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGebbddjqBIn","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413028,"user_tz":420,"elapsed":76,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["checkpoint_path = \"gdrive/My Drive/ML/Projects/CSGO_aim/save/training_1/cp.ckpt\"\n","\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n","\n","\n","def train(steps):\n","  optimizer = tf.keras.optimizers.Adam(1e-4)\n"," \n","  for n in range(steps):\n","    with tf.GradientTape(persistent=True) as tape:\n","      for img_batch, labels in train_dataset.take(1):\n","        loss = 0\n","        for img, label in zip(img_batch, labels):\n","          img = tf.expand_dims(img, 0)\n","          #with tf.GradientTape() as tape: \n","          outputs = model(img)\n","          loss += compute_loss(outputs[0], outputs[1], img, label.numpy(), s)\n","    \n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    gradients = [grad if grad is not None else tf.zeros_like(var)\n","      for var, grad in zip(model.trainable_variables, gradients)]\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","    if n % 50 == 0:\n","      model.save(checkpoint_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gbbsbHDTQlK","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413029,"user_tz":420,"elapsed":67,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["train(2000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cl_CbuQgidGN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":218},"executionInfo":{"status":"ok","timestamp":1592953566981,"user_tz":420,"elapsed":1928,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}},"outputId":"2bd3b90a-ed80-48c8-8457-7de69c22fd8f"},"source":["for img_batch, labels in train_dataset.take(1):\n","\n","  img = img_batch[0]\n","  \n","  img = tf.expand_dims(img, 0)\n","  output_1, output_2 = model(img)\n","\n","  # print(output_1[0][0][0][:5]) # bounding box #1 5 predictions\n","  # print(output_2[0][1][2][0])\n","\n","  for row in range(s):\n","    for col in range(s):\n","      # Confident that there is an object in there\n","      if output_1[0][row][col][4] > 0:\n","        print('Model is confident', output_1[0][row][col][4])\n","        model_coords = output_1[0][row][col][:4]\n","        true_y = row * (IMG_HEIGHT // s) + (tf.clip_by_value(model_coords[0], 0, 1) ** 2) * (IMG_HEIGHT // s)\n","        true_x = col * (IMG_WIDTH // s) + (tf.clip_by_value(model_coords[1], 0, 1) ** 2) * (IMG_WIDTH // s) \n","        print(true_x, true_y)\n","        #print(output_1[0][row][col][:4])\n","\n","  print(labels[0].numpy())\n","\n","  #print(get_responsible_grids(0, labels[0].numpy(), s))\n","\n","  #print(labels[0].numpy())\n","\n"],"execution_count":76,"outputs":[{"output_type":"stream","text":["Model is confident tf.Tensor(0.031476207, shape=(), dtype=float32)\n","tf.Tensor(298.1684, shape=(), dtype=float32) tf.Tensor(151.32822, shape=(), dtype=float32)\n","[[211 113 266 254   1]\n"," [260 139 302 227   1]\n"," [ -1  -1  -1  -1  -1]\n"," [ -1  -1  -1  -1  -1]\n"," [ -1  -1  -1  -1  -1]\n"," [ -1  -1  -1  -1  -1]\n"," [ -1  -1  -1  -1  -1]\n"," [ -1  -1  -1  -1  -1]\n"," [ -1  -1  -1  -1  -1]\n"," [ -1  -1  -1  -1  -1]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FEtPqW_aVlqX","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413029,"user_tz":420,"elapsed":47,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"scRyJ0SNOTa4","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1592953413030,"user_tz":420,"elapsed":39,"user":{"displayName":"mg nt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0-pYHGP9CbFA2AZRp8OnLpMhdCJ9Okp_WJWg7=s64","userId":"03098984538835906155"}}},"source":["import cv2\n","\n","img = cv2.imread('gdrive/My Drive/ML/Projects/CSGO_aim/images/1.jpg')\n","img"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8tMZDGhSAAn","colab_type":"text"},"source":[""]}]}